#!/usr/bin/env python
import requests
import sys
import os
import re
import time
import urllib.request
from bs4 import BeautifulSoup



def download(url,path,pageNumber,ref):
  if not os.path.exists(path):
    print ('文件夹', path, '不存在，重新建立')
    os.makedirs(path)
    newUrl = url.split('1.jpg')[0]
    for i in range(1,int(pageNumber)+1):
        print(newUrl+str(i)+'.jpg')
        opener = urllib.request.build_opener()
        opener.addheaders = [('Referer',ref ),('User-Agent',heads('UserAgent'))]
        urllib.request.install_opener(opener)
        urllib.request.urlretrieve(newUrl+str(i)+'.jpg',path+str(i)+'.jpg')
        print('下载完成'+str(i)+'jpg')

def getTargetDiv(url):
    html = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(html,"html.parser")
    title = soup.find_all({"h2","h3","h4","h5","h6"})
    txttitle = title[0].text                                    #获取纯文本标题内容
    div = soup.find_all('div', attrs={'class':'content-pic'})
    span =soup.find_all('span',attrs={'class':'page-ch'})       #获取图片页数html页面
    pattern = re.compile(r'\d+').findall(str(span[0]))          #获取图片总页数列表
    pageNumber = pattern[0]
    for i in div:
        getType = i.find('img')
        pic_link = getType.get('src')
    return txttitle,pic_link,pageNumber


def write_log(path,contents,url):
    writetime=time.strftime("%Y-%m-%d %H:%M", time.localtime())
    with open(path, 'a') as filew:
        filew.write(writetime+' '+url+' '+str(contents))
        filew.write('\n')
def heads(var):
    dictList = {'base_domin':'http://mm131.com/xinggan/',
                'reFereR' : 'http://www.mm131.com/xinggan/1002.html',
                'UserAgent' : 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'}
    return (dictList.get(var))

def man():

    for i in range(10000):
         if i < 1002:
            continue
         urll = heads('base_domin')+str(i)+".html"
         print(urll)
         r = requests.get(urll,headers=hearder)
         if not r.status_code == 404:
             print(urll,r.status_code)
             txtt = getTargetDiv(urll)
             download(txtt[1],path+txtt[0]+'\\',txtt[2],urll)
         elif r.status_code == 404:
             write_log(path+'run.txt',str(r.status_code),urll)
path = 'D:\mm131\\qipao\\'
hearder = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'}
if __name__ == '__main__':
    man()




